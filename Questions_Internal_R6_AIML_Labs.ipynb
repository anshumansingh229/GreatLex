{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Questions - Internal - R6 - AIML Labs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUZjPnVXGz0Z",
        "colab_type": "text"
      },
      "source": [
        "# The Iris Dataset\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMbmpriavLE9",
        "colab_type": "text"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fu8bUU__oa7h",
        "outputId": "9c55a488-8ace-45ad-e8f2-af17722ac0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip3 install -U tensorflow-gpu --quiet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 380.8MB 58kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 53.8MB/s \n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLz1Ckvfvn6D"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWrzVTLOvn6M",
        "outputId": "f844542f-4604-45a5-b5c9-20a7fde03b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uYeJgkNuXNC",
        "colab_type": "text"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcASNsewsfQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vVQBBqg7DI",
        "colab_type": "text"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0EDKvQhEIe",
        "colab_type": "text"
      },
      "source": [
        "### Import dataset\n",
        "- Import iris dataset\n",
        "- Import the dataset using sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOWpD26Haq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJFL0IaxcS9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0cf610f9-93bc-46d6-d2b5-404304a8f992"
      },
      "source": [
        "type(iris)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.utils.Bunch"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8YqInTh5v5",
        "colab_type": "text"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HERt3drbhX0i",
        "colab_type": "text"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- you can get the features using .data method\n",
        "- you can get the features using .target method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cV-_qHAHyvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris.data \n",
        "y = iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHsSTEaxe3k-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7365c2d4-b230-47d9-d50d-d3aa50a4902c"
      },
      "source": [
        "X"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JQziMLZe5mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2dded594-409a-4785-d458-6c61f6848439"
      },
      "source": [
        "y"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1A2lkUjFak",
        "colab_type": "text"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### Create train and test data\n",
        "- use train_test_split to get train and test set\n",
        "- set a random_state\n",
        "- test_size: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKNJL85h7pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q0RFgm8huR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2FDcfNnh-jl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "64b5e5b5-5d4c-41b3-ca29-04ac08306e19"
      },
      "source": [
        "X_test"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.1, 2.8, 4.7, 1.2],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [5.2, 3.5, 1.5, 0.2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_bbVXmyh-we",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f2d4c8ab-8029-456c-fbe3-ef6049a2e5a2"
      },
      "source": [
        "iris.target"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0KVP17Ozaix",
        "colab_type": "text"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjqxbhWv1zv",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encode the labels\n",
        "- convert class vectors (integers) to binary class matrix\n",
        "- convert labels\n",
        "- number of classes: 3\n",
        "- we are doing this to use categorical_crossentropy as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9vv-_gpyLY9",
        "colab": {}
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovjLyYzWkO9s"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIFzoPNSyYo",
        "colab_type": "text"
      },
      "source": [
        "### Initialize a sequential model\n",
        "- Define a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvSbf1UjHtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dGMy999vlacX"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ibK5Jxm8iL",
        "colab_type": "text"
      },
      "source": [
        "### Add a layer\n",
        "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
        "- Apply Softmax on Dense Layer outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKrBNSRm_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.add(tf.keras.layers.Reshape((784,),input_shape=(4,4,)))\n",
        "#Add Dense Layer which provides 3 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax', input_shape=(4,)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4uiTH8plmNX",
        "colab_type": "text"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJL8n8vcSyYz",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "- Use SGD as Optimizer\n",
        "- Use categorical_crossentropy as loss function\n",
        "- Use accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_-fjIEk1ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sihIGbRll_jT"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZZCfNGlu0i",
        "colab_type": "text"
      },
      "source": [
        "### Summarize the model\n",
        "- Check model layers\n",
        "- Understand number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elER3F_4ln8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "d89de52d-22c4-412c-f590-21fd5d426381"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 15\n",
            "Trainable params: 15\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2PiP7j3Vmj4p"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWdbfFCXmCHt",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model\n",
        "- Give train data as training features and labels\n",
        "- Epochs: 100\n",
        "- Give validation data as testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO1c-5tjmBVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffe696fb-1ca8-4f88-d835-f08c73643898"
      },
      "source": [
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_test, y_test), \n",
        "          epochs=100,\n",
        "          batch_size=32)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 112 samples, validate on 38 samples\n",
            "Epoch 1/100\n",
            "112/112 [==============================] - 1s 6ms/sample - loss: 2.7632 - accuracy: 0.2857 - val_loss: 2.6195 - val_accuracy: 0.1579\n",
            "Epoch 2/100\n",
            "112/112 [==============================] - 0s 199us/sample - loss: 2.3102 - accuracy: 0.1607 - val_loss: 2.3138 - val_accuracy: 0.1316\n",
            "Epoch 3/100\n",
            "112/112 [==============================] - 0s 219us/sample - loss: 2.0531 - accuracy: 0.0893 - val_loss: 2.1683 - val_accuracy: 0.1579\n",
            "Epoch 4/100\n",
            "112/112 [==============================] - 0s 210us/sample - loss: 1.9417 - accuracy: 0.1339 - val_loss: 2.0779 - val_accuracy: 0.1842\n",
            "Epoch 5/100\n",
            "112/112 [==============================] - 0s 214us/sample - loss: 1.8797 - accuracy: 0.1875 - val_loss: 2.0155 - val_accuracy: 0.2105\n",
            "Epoch 6/100\n",
            "112/112 [==============================] - 0s 230us/sample - loss: 1.8211 - accuracy: 0.2589 - val_loss: 1.9559 - val_accuracy: 0.2368\n",
            "Epoch 7/100\n",
            "112/112 [==============================] - 0s 217us/sample - loss: 1.7792 - accuracy: 0.2321 - val_loss: 1.8926 - val_accuracy: 0.2632\n",
            "Epoch 8/100\n",
            "112/112 [==============================] - 0s 216us/sample - loss: 1.7346 - accuracy: 0.3214 - val_loss: 1.8625 - val_accuracy: 0.2368\n",
            "Epoch 9/100\n",
            "112/112 [==============================] - 0s 190us/sample - loss: 1.6868 - accuracy: 0.1964 - val_loss: 1.7958 - val_accuracy: 0.2368\n",
            "Epoch 10/100\n",
            "112/112 [==============================] - 0s 218us/sample - loss: 1.6355 - accuracy: 0.3125 - val_loss: 1.7374 - val_accuracy: 0.1842\n",
            "Epoch 11/100\n",
            "112/112 [==============================] - 0s 213us/sample - loss: 1.5899 - accuracy: 0.2500 - val_loss: 1.6854 - val_accuracy: 0.2368\n",
            "Epoch 12/100\n",
            "112/112 [==============================] - 0s 206us/sample - loss: 1.5491 - accuracy: 0.2679 - val_loss: 1.6310 - val_accuracy: 0.2632\n",
            "Epoch 13/100\n",
            "112/112 [==============================] - 0s 199us/sample - loss: 1.5103 - accuracy: 0.2946 - val_loss: 1.5782 - val_accuracy: 0.2632\n",
            "Epoch 14/100\n",
            "112/112 [==============================] - 0s 209us/sample - loss: 1.4633 - accuracy: 0.2857 - val_loss: 1.5296 - val_accuracy: 0.2895\n",
            "Epoch 15/100\n",
            "112/112 [==============================] - 0s 207us/sample - loss: 1.4250 - accuracy: 0.3304 - val_loss: 1.4777 - val_accuracy: 0.2632\n",
            "Epoch 16/100\n",
            "112/112 [==============================] - 0s 204us/sample - loss: 1.3867 - accuracy: 0.2946 - val_loss: 1.4412 - val_accuracy: 0.2632\n",
            "Epoch 17/100\n",
            "112/112 [==============================] - 0s 216us/sample - loss: 1.3508 - accuracy: 0.2946 - val_loss: 1.3934 - val_accuracy: 0.3158\n",
            "Epoch 18/100\n",
            "112/112 [==============================] - 0s 229us/sample - loss: 1.3180 - accuracy: 0.3482 - val_loss: 1.3558 - val_accuracy: 0.2632\n",
            "Epoch 19/100\n",
            "112/112 [==============================] - 0s 233us/sample - loss: 1.2816 - accuracy: 0.4018 - val_loss: 1.3123 - val_accuracy: 0.2895\n",
            "Epoch 20/100\n",
            "112/112 [==============================] - 0s 225us/sample - loss: 1.2459 - accuracy: 0.3304 - val_loss: 1.2806 - val_accuracy: 0.3158\n",
            "Epoch 21/100\n",
            "112/112 [==============================] - 0s 195us/sample - loss: 1.2171 - accuracy: 0.3393 - val_loss: 1.2419 - val_accuracy: 0.3421\n",
            "Epoch 22/100\n",
            "112/112 [==============================] - 0s 202us/sample - loss: 1.1848 - accuracy: 0.3036 - val_loss: 1.1965 - val_accuracy: 0.2895\n",
            "Epoch 23/100\n",
            "112/112 [==============================] - 0s 217us/sample - loss: 1.1553 - accuracy: 0.2857 - val_loss: 1.1580 - val_accuracy: 0.3421\n",
            "Epoch 24/100\n",
            "112/112 [==============================] - 0s 219us/sample - loss: 1.1299 - accuracy: 0.3929 - val_loss: 1.1269 - val_accuracy: 0.3684\n",
            "Epoch 25/100\n",
            "112/112 [==============================] - 0s 223us/sample - loss: 1.1012 - accuracy: 0.4643 - val_loss: 1.0931 - val_accuracy: 0.3158\n",
            "Epoch 26/100\n",
            "112/112 [==============================] - 0s 210us/sample - loss: 1.0718 - accuracy: 0.3214 - val_loss: 1.0639 - val_accuracy: 0.3158\n",
            "Epoch 27/100\n",
            "112/112 [==============================] - 0s 225us/sample - loss: 1.0557 - accuracy: 0.3482 - val_loss: 1.0385 - val_accuracy: 0.3421\n",
            "Epoch 28/100\n",
            "112/112 [==============================] - 0s 233us/sample - loss: 1.0270 - accuracy: 0.3214 - val_loss: 1.0117 - val_accuracy: 0.3684\n",
            "Epoch 29/100\n",
            "112/112 [==============================] - 0s 192us/sample - loss: 1.0040 - accuracy: 0.3393 - val_loss: 0.9912 - val_accuracy: 0.4474\n",
            "Epoch 30/100\n",
            "112/112 [==============================] - 0s 202us/sample - loss: 0.9835 - accuracy: 0.5357 - val_loss: 0.9623 - val_accuracy: 0.5000\n",
            "Epoch 31/100\n",
            "112/112 [==============================] - 0s 193us/sample - loss: 0.9647 - accuracy: 0.5268 - val_loss: 0.9440 - val_accuracy: 0.5263\n",
            "Epoch 32/100\n",
            "112/112 [==============================] - 0s 241us/sample - loss: 0.9444 - accuracy: 0.5268 - val_loss: 0.9222 - val_accuracy: 0.6579\n",
            "Epoch 33/100\n",
            "112/112 [==============================] - 0s 209us/sample - loss: 0.9282 - accuracy: 0.6071 - val_loss: 0.9045 - val_accuracy: 0.7368\n",
            "Epoch 34/100\n",
            "112/112 [==============================] - 0s 199us/sample - loss: 0.9098 - accuracy: 0.6607 - val_loss: 0.8874 - val_accuracy: 0.8158\n",
            "Epoch 35/100\n",
            "112/112 [==============================] - 0s 216us/sample - loss: 0.8953 - accuracy: 0.6964 - val_loss: 0.8648 - val_accuracy: 0.7895\n",
            "Epoch 36/100\n",
            "112/112 [==============================] - 0s 210us/sample - loss: 0.8889 - accuracy: 0.7232 - val_loss: 0.8581 - val_accuracy: 0.8158\n",
            "Epoch 37/100\n",
            "112/112 [==============================] - 0s 210us/sample - loss: 0.8694 - accuracy: 0.7768 - val_loss: 0.8334 - val_accuracy: 0.7368\n",
            "Epoch 38/100\n",
            "112/112 [==============================] - 0s 196us/sample - loss: 0.8530 - accuracy: 0.6696 - val_loss: 0.8152 - val_accuracy: 0.7632\n",
            "Epoch 39/100\n",
            "112/112 [==============================] - 0s 179us/sample - loss: 0.8364 - accuracy: 0.7500 - val_loss: 0.8004 - val_accuracy: 0.7368\n",
            "Epoch 40/100\n",
            "112/112 [==============================] - 0s 219us/sample - loss: 0.8244 - accuracy: 0.7143 - val_loss: 0.7863 - val_accuracy: 0.7632\n",
            "Epoch 41/100\n",
            "112/112 [==============================] - 0s 220us/sample - loss: 0.8116 - accuracy: 0.6964 - val_loss: 0.7732 - val_accuracy: 0.7895\n",
            "Epoch 42/100\n",
            "112/112 [==============================] - 0s 201us/sample - loss: 0.7987 - accuracy: 0.7500 - val_loss: 0.7609 - val_accuracy: 0.7895\n",
            "Epoch 43/100\n",
            "112/112 [==============================] - 0s 260us/sample - loss: 0.7894 - accuracy: 0.6875 - val_loss: 0.7488 - val_accuracy: 0.7632\n",
            "Epoch 44/100\n",
            "112/112 [==============================] - 0s 215us/sample - loss: 0.7797 - accuracy: 0.7679 - val_loss: 0.7404 - val_accuracy: 0.7368\n",
            "Epoch 45/100\n",
            "112/112 [==============================] - 0s 226us/sample - loss: 0.7714 - accuracy: 0.6518 - val_loss: 0.7299 - val_accuracy: 0.8421\n",
            "Epoch 46/100\n",
            "112/112 [==============================] - 0s 201us/sample - loss: 0.7605 - accuracy: 0.7321 - val_loss: 0.7207 - val_accuracy: 0.8947\n",
            "Epoch 47/100\n",
            "112/112 [==============================] - 0s 188us/sample - loss: 0.7505 - accuracy: 0.7946 - val_loss: 0.7068 - val_accuracy: 0.7895\n",
            "Epoch 48/100\n",
            "112/112 [==============================] - 0s 186us/sample - loss: 0.7418 - accuracy: 0.7232 - val_loss: 0.7002 - val_accuracy: 0.8947\n",
            "Epoch 49/100\n",
            "112/112 [==============================] - 0s 218us/sample - loss: 0.7340 - accuracy: 0.7946 - val_loss: 0.6854 - val_accuracy: 0.7895\n",
            "Epoch 50/100\n",
            "112/112 [==============================] - 0s 198us/sample - loss: 0.7226 - accuracy: 0.7500 - val_loss: 0.6739 - val_accuracy: 0.7895\n",
            "Epoch 51/100\n",
            "112/112 [==============================] - 0s 209us/sample - loss: 0.7172 - accuracy: 0.6964 - val_loss: 0.6691 - val_accuracy: 0.8158\n",
            "Epoch 52/100\n",
            "112/112 [==============================] - 0s 236us/sample - loss: 0.7074 - accuracy: 0.8036 - val_loss: 0.6593 - val_accuracy: 0.7895\n",
            "Epoch 53/100\n",
            "112/112 [==============================] - 0s 196us/sample - loss: 0.7034 - accuracy: 0.6786 - val_loss: 0.6518 - val_accuracy: 0.7895\n",
            "Epoch 54/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 0.6962 - accuracy: 0.7232 - val_loss: 0.6471 - val_accuracy: 0.8158\n",
            "Epoch 55/100\n",
            "112/112 [==============================] - 0s 181us/sample - loss: 0.6878 - accuracy: 0.8304 - val_loss: 0.6374 - val_accuracy: 0.8158\n",
            "Epoch 56/100\n",
            "112/112 [==============================] - 0s 241us/sample - loss: 0.6836 - accuracy: 0.7768 - val_loss: 0.6298 - val_accuracy: 0.8421\n",
            "Epoch 57/100\n",
            "112/112 [==============================] - 0s 207us/sample - loss: 0.6746 - accuracy: 0.7411 - val_loss: 0.6229 - val_accuracy: 0.8421\n",
            "Epoch 58/100\n",
            "112/112 [==============================] - 0s 203us/sample - loss: 0.6674 - accuracy: 0.7946 - val_loss: 0.6163 - val_accuracy: 0.8421\n",
            "Epoch 59/100\n",
            "112/112 [==============================] - 0s 195us/sample - loss: 0.6655 - accuracy: 0.7946 - val_loss: 0.6109 - val_accuracy: 0.8421\n",
            "Epoch 60/100\n",
            "112/112 [==============================] - 0s 222us/sample - loss: 0.6599 - accuracy: 0.7946 - val_loss: 0.6028 - val_accuracy: 0.8421\n",
            "Epoch 61/100\n",
            "112/112 [==============================] - 0s 180us/sample - loss: 0.6527 - accuracy: 0.7411 - val_loss: 0.6011 - val_accuracy: 0.9211\n",
            "Epoch 62/100\n",
            "112/112 [==============================] - 0s 215us/sample - loss: 0.6477 - accuracy: 0.8393 - val_loss: 0.5955 - val_accuracy: 0.9211\n",
            "Epoch 63/100\n",
            "112/112 [==============================] - 0s 230us/sample - loss: 0.6408 - accuracy: 0.8750 - val_loss: 0.5884 - val_accuracy: 0.8421\n",
            "Epoch 64/100\n",
            "112/112 [==============================] - 0s 200us/sample - loss: 0.6378 - accuracy: 0.8036 - val_loss: 0.5838 - val_accuracy: 0.8684\n",
            "Epoch 65/100\n",
            "112/112 [==============================] - 0s 260us/sample - loss: 0.6378 - accuracy: 0.8036 - val_loss: 0.5802 - val_accuracy: 0.9211\n",
            "Epoch 66/100\n",
            "112/112 [==============================] - 0s 218us/sample - loss: 0.6270 - accuracy: 0.8661 - val_loss: 0.5718 - val_accuracy: 0.8158\n",
            "Epoch 67/100\n",
            "112/112 [==============================] - 0s 219us/sample - loss: 0.6225 - accuracy: 0.8214 - val_loss: 0.5672 - val_accuracy: 0.8421\n",
            "Epoch 68/100\n",
            "112/112 [==============================] - 0s 211us/sample - loss: 0.6199 - accuracy: 0.7946 - val_loss: 0.5641 - val_accuracy: 0.8421\n",
            "Epoch 69/100\n",
            "112/112 [==============================] - 0s 206us/sample - loss: 0.6173 - accuracy: 0.8125 - val_loss: 0.5590 - val_accuracy: 0.7895\n",
            "Epoch 70/100\n",
            "112/112 [==============================] - 0s 200us/sample - loss: 0.6122 - accuracy: 0.7679 - val_loss: 0.5545 - val_accuracy: 0.7895\n",
            "Epoch 71/100\n",
            "112/112 [==============================] - 0s 211us/sample - loss: 0.6148 - accuracy: 0.7232 - val_loss: 0.5496 - val_accuracy: 0.8421\n",
            "Epoch 72/100\n",
            "112/112 [==============================] - 0s 211us/sample - loss: 0.6051 - accuracy: 0.8304 - val_loss: 0.5457 - val_accuracy: 0.7895\n",
            "Epoch 73/100\n",
            "112/112 [==============================] - 0s 212us/sample - loss: 0.5994 - accuracy: 0.7679 - val_loss: 0.5424 - val_accuracy: 0.7895\n",
            "Epoch 74/100\n",
            "112/112 [==============================] - 0s 233us/sample - loss: 0.5958 - accuracy: 0.7321 - val_loss: 0.5390 - val_accuracy: 0.8421\n",
            "Epoch 75/100\n",
            "112/112 [==============================] - 0s 263us/sample - loss: 0.5916 - accuracy: 0.8393 - val_loss: 0.5365 - val_accuracy: 0.8684\n",
            "Epoch 76/100\n",
            "112/112 [==============================] - 0s 215us/sample - loss: 0.5921 - accuracy: 0.8750 - val_loss: 0.5333 - val_accuracy: 0.7895\n",
            "Epoch 77/100\n",
            "112/112 [==============================] - 0s 213us/sample - loss: 0.5854 - accuracy: 0.7411 - val_loss: 0.5312 - val_accuracy: 0.8684\n",
            "Epoch 78/100\n",
            "112/112 [==============================] - 0s 223us/sample - loss: 0.5831 - accuracy: 0.8661 - val_loss: 0.5277 - val_accuracy: 0.8684\n",
            "Epoch 79/100\n",
            "112/112 [==============================] - 0s 210us/sample - loss: 0.5837 - accuracy: 0.8304 - val_loss: 0.5225 - val_accuracy: 0.8684\n",
            "Epoch 80/100\n",
            "112/112 [==============================] - 0s 213us/sample - loss: 0.5744 - accuracy: 0.8482 - val_loss: 0.5189 - val_accuracy: 0.8158\n",
            "Epoch 81/100\n",
            "112/112 [==============================] - 0s 195us/sample - loss: 0.5719 - accuracy: 0.8036 - val_loss: 0.5167 - val_accuracy: 0.8684\n",
            "Epoch 82/100\n",
            "112/112 [==============================] - 0s 214us/sample - loss: 0.5722 - accuracy: 0.8393 - val_loss: 0.5135 - val_accuracy: 0.8421\n",
            "Epoch 83/100\n",
            "112/112 [==============================] - 0s 201us/sample - loss: 0.5671 - accuracy: 0.8214 - val_loss: 0.5105 - val_accuracy: 0.8684\n",
            "Epoch 84/100\n",
            "112/112 [==============================] - 0s 216us/sample - loss: 0.5630 - accuracy: 0.8214 - val_loss: 0.5092 - val_accuracy: 0.9474\n",
            "Epoch 85/100\n",
            "112/112 [==============================] - 0s 210us/sample - loss: 0.5611 - accuracy: 0.9107 - val_loss: 0.5047 - val_accuracy: 0.8684\n",
            "Epoch 86/100\n",
            "112/112 [==============================] - 0s 211us/sample - loss: 0.5594 - accuracy: 0.8393 - val_loss: 0.5040 - val_accuracy: 0.7632\n",
            "Epoch 87/100\n",
            "112/112 [==============================] - 0s 220us/sample - loss: 0.5634 - accuracy: 0.8125 - val_loss: 0.5022 - val_accuracy: 0.7632\n",
            "Epoch 88/100\n",
            "112/112 [==============================] - 0s 276us/sample - loss: 0.5555 - accuracy: 0.7500 - val_loss: 0.4976 - val_accuracy: 0.8158\n",
            "Epoch 89/100\n",
            "112/112 [==============================] - 0s 209us/sample - loss: 0.5541 - accuracy: 0.8304 - val_loss: 0.4958 - val_accuracy: 0.7895\n",
            "Epoch 90/100\n",
            "112/112 [==============================] - 0s 234us/sample - loss: 0.5493 - accuracy: 0.7321 - val_loss: 0.4941 - val_accuracy: 0.9474\n",
            "Epoch 91/100\n",
            "112/112 [==============================] - 0s 217us/sample - loss: 0.5495 - accuracy: 0.8929 - val_loss: 0.4894 - val_accuracy: 0.8684\n",
            "Epoch 92/100\n",
            "112/112 [==============================] - 0s 217us/sample - loss: 0.5451 - accuracy: 0.8482 - val_loss: 0.4886 - val_accuracy: 0.9474\n",
            "Epoch 93/100\n",
            "112/112 [==============================] - 0s 203us/sample - loss: 0.5418 - accuracy: 0.9196 - val_loss: 0.4846 - val_accuracy: 0.8421\n",
            "Epoch 94/100\n",
            "112/112 [==============================] - 0s 196us/sample - loss: 0.5444 - accuracy: 0.8304 - val_loss: 0.4822 - val_accuracy: 0.8684\n",
            "Epoch 95/100\n",
            "112/112 [==============================] - 0s 245us/sample - loss: 0.5362 - accuracy: 0.8393 - val_loss: 0.4808 - val_accuracy: 0.9211\n",
            "Epoch 96/100\n",
            "112/112 [==============================] - 0s 205us/sample - loss: 0.5368 - accuracy: 0.8750 - val_loss: 0.4770 - val_accuracy: 0.8684\n",
            "Epoch 97/100\n",
            "112/112 [==============================] - 0s 205us/sample - loss: 0.5316 - accuracy: 0.8482 - val_loss: 0.4749 - val_accuracy: 0.8684\n",
            "Epoch 98/100\n",
            "112/112 [==============================] - 0s 215us/sample - loss: 0.5293 - accuracy: 0.8571 - val_loss: 0.4729 - val_accuracy: 0.8684\n",
            "Epoch 99/100\n",
            "112/112 [==============================] - 0s 235us/sample - loss: 0.5271 - accuracy: 0.8571 - val_loss: 0.4712 - val_accuracy: 0.9211\n",
            "Epoch 100/100\n",
            "112/112 [==============================] - 0s 198us/sample - loss: 0.5304 - accuracy: 0.8571 - val_loss: 0.4702 - val_accuracy: 0.9474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa54f189390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9ItAR3yS3J",
        "colab_type": "text"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw0IFf9yVqH",
        "colab_type": "text"
      },
      "source": [
        "### Make predictions\n",
        "- Predict labels on one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5sBybi6mlLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "deb98857-3043-4f68-da42-ae6c6518a385"
      },
      "source": [
        "import numpy as np\n",
        "input_data = np.expand_dims(X_test[0], axis=0)\n",
        "print(input_data.shape)\n",
        "pred = model.predict(input_data)\n",
        "pred"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08060874, 0.55502105, 0.36437026]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLs8skcdmuWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "edd89cd5-59cd-43a2-ccc3-5bafa58d2e3a"
      },
      "source": [
        "pred[0]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.08060874, 0.55502105, 0.36437026], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKFKnn0amvPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "68682b12-bb96-41da-b259-f98276304e44"
      },
      "source": [
        "np.argmax(pred[0])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSUgMq3m0bG7",
        "colab_type": "text"
      },
      "source": [
        "### Compare the prediction with actual label\n",
        "- Print the same row as done in the previous step but of actual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWz_CqiTmtKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets print the image as well\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plt.imshow(X_test[0],cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5WbwVPyz-qQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrTKwbgE7NFT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1UBYPNp5Tn1",
        "colab_type": "text"
      },
      "source": [
        "# Stock prices dataset\n",
        "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
        "\n",
        "## Description\n",
        "A brief description of columns.\n",
        "- open: The opening market price of the equity symbol on the date\n",
        "- high: The highest market price of the equity symbol on the date\n",
        "- low: The lowest recorded market price of the equity symbol on the date\n",
        "- close: The closing recorded price of the equity symbol on the date\n",
        "- symbol: Symbol of the listed company\n",
        "- volume: Total traded volume of the equity symbol on the date\n",
        "- date: Date of record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctH_ZW5g-M3g"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vQbdODpH-M3r",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nFQWH1tj-M38"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ho5n-xhd-M3_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8adfdf93-423d-4657-ebe4-7c96fc7c96a3"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgkl0qu6-M4F"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKgTyuA3-M4G",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_88voqAH-O6J",
        "colab_type": "text"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRHCeJqP-evf",
        "colab_type": "text"
      },
      "source": [
        "### Load the data\n",
        "- load the csv file and read it using pandas\n",
        "- file name is prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKVH5v7r-RmC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "951123d7-d437-44ab-fb6e-f64c46850cdb"
      },
      "source": [
        "# run this cell to upload file if you are using google colab\n",
        "from google.colab import files\n",
        "#files.upload()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/content/gdrive/My Drive/Collab/prices.csv')\n",
        "#os.chdir('/content/gdrive/My Drive/Collab/')"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gDC6cSW_FSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "0319a5b5-f9da-420a-a7ba-96d69de59314"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlLKVPVH_BCT",
        "colab_type": "text"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4BlzVA_gZd",
        "colab_type": "text"
      },
      "source": [
        "### Drop columnns\n",
        "- drop \"date\" and \"symbol\" column from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKEK8aEE_Csx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "790e4167-4e16-49c9-ccec-1cc87675fd6b"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2WW4PyCqvXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9deb0651-242a-4d7e-a179-478bbac31f31"
      },
      "source": [
        "df_date = df['date']\n",
        "df = df.drop('date', axis=1)\n",
        "df.columns"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['symbol', 'open', 'close', 'low', 'high', 'volume'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M8KYgkuq7lq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da8f172e-adb2-4ec6-fcc5-4523d74740bc"
      },
      "source": [
        "df_symbol = df['symbol']\n",
        "df = df.drop('symbol', axis=1)\n",
        "df.columns"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['open', 'close', 'low', 'high', 'volume'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0Am2SDKrUHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "661e36bd-e63e-4064-c679-a412a5bbb85c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBuYw65mrha6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad25cfcc-fd05-4859-ce9e-c73c28bd6f43"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(851264, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKkVHFiIrhpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1278826c-7e00-41be-859e-efb2909025d2"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "open      float64\n",
              "close     float64\n",
              "low       float64\n",
              "high      float64\n",
              "volume    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPhO6v-AiZt",
        "colab_type": "text"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZXmF3NAkna",
        "colab_type": "text"
      },
      "source": [
        "### Take initial rows\n",
        "- Take first 1000 rows from the data\n",
        "- This step is done to make the execution faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKs04iIHAjxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.head(1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRgwRs-Ir5c8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3ec3a97-f21f-4137-a0ad-c03093d1d91e"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1v35xh9r5nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGtnapgBIJm",
        "colab_type": "text"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8u_jlbABTip"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
        "- Take \"volume\" column as label\n",
        "- Normalize label column by dividing it with 1000000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjCMzUXBJbg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67a4cb3d-eada-4bcc-9fe1-036bb2d3283c"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['open', 'close', 'low', 'high', 'volume'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LOqp3EBsRK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a7e3bf9-09c7-4122-c0ea-8a5250b265e6"
      },
      "source": [
        "feature = df.iloc[:,0:4]\n",
        "feature.columns"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['open', 'close', 'low', 'high'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-K6vpqisKxV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7181b264-c9d6-4999-af4b-0cec467e255c"
      },
      "source": [
        "labels = df['volume']\n",
        "labels"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2163600.0\n",
              "1       2386400.0\n",
              "2       2489500.0\n",
              "3       2006300.0\n",
              "4       1408600.0\n",
              "5       1098000.0\n",
              "6        949600.0\n",
              "7        785300.0\n",
              "8       1093700.0\n",
              "9       1523500.0\n",
              "10      1653900.0\n",
              "11       944300.0\n",
              "12       744900.0\n",
              "13       703800.0\n",
              "14       563100.0\n",
              "15       896100.0\n",
              "16       680400.0\n",
              "17       749900.0\n",
              "18       574200.0\n",
              "19       694800.0\n",
              "20       896300.0\n",
              "21       956300.0\n",
              "22       997100.0\n",
              "23      1200500.0\n",
              "24      1725200.0\n",
              "25      1946000.0\n",
              "26      1319500.0\n",
              "27       922400.0\n",
              "28      1185100.0\n",
              "29       921500.0\n",
              "          ...    \n",
              "970     9097500.0\n",
              "971     2242100.0\n",
              "972     6205100.0\n",
              "973      864600.0\n",
              "974      709200.0\n",
              "975     1129600.0\n",
              "976     3449300.0\n",
              "977     7517100.0\n",
              "978     2356500.0\n",
              "979     4839200.0\n",
              "980     3684600.0\n",
              "981    14428400.0\n",
              "982     1463300.0\n",
              "983      749600.0\n",
              "984    13355100.0\n",
              "985    11538300.0\n",
              "986     9931300.0\n",
              "987    12906000.0\n",
              "988      369900.0\n",
              "989     2701000.0\n",
              "990     2627800.0\n",
              "991     3258700.0\n",
              "992     5206100.0\n",
              "993     7099000.0\n",
              "994     7795500.0\n",
              "995     2133200.0\n",
              "996     1982400.0\n",
              "997    37152800.0\n",
              "998     6568600.0\n",
              "999     5604300.0\n",
              "Name: volume, Length: 1000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoZLMgyfvBZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "10652416-ccd5-4b75-cddf-666f2e529d3c"
      },
      "source": [
        "labels = labels.divide(1000)\n",
        "labels.head()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2163.6\n",
              "1    2386.4\n",
              "2    2489.5\n",
              "3    2006.3\n",
              "4    1408.6\n",
              "Name: volume, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTAKzlxZBz0z",
        "colab_type": "text"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfY8Km1Zzyt2",
        "colab_type": "text"
      },
      "source": [
        "### Convert data\n",
        "- Convert features and labels to numpy array\n",
        "- Convert their data type to \"float32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko7nnQVbYENh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = feature.to_numpy()\n",
        "labels = labels.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E1faX0qwIvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = feature.astype('float32')\n",
        "labels = labels.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjE08eJEwVTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d3d4abdc-0de1-48cd-e22a-0ce9ec53eaec"
      },
      "source": [
        "feature"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[123.43, 125.84, 122.31, 126.25],\n",
              "       [125.24, 119.98, 119.94, 125.54],\n",
              "       [116.38, 114.95, 114.93, 119.74],\n",
              "       ...,\n",
              "       [ 28.32,  28.77,  28.01,  28.81],\n",
              "       [ 44.  ,  44.8 ,  43.75,  44.81],\n",
              "       [ 36.08,  37.14,  36.01,  37.23]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2sLoHBYwXcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43fc328e-d0ad-435d-a5d0-0009b0e63851"
      },
      "source": [
        "labels"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.163600e+03, 2.386400e+03, 2.489500e+03, 2.006300e+03,\n",
              "       1.408600e+03, 1.098000e+03, 9.496000e+02, 7.853000e+02,\n",
              "       1.093700e+03, 1.523500e+03, 1.653900e+03, 9.443000e+02,\n",
              "       7.449000e+02, 7.038000e+02, 5.631000e+02, 8.961000e+02,\n",
              "       6.804000e+02, 7.499000e+02, 5.742000e+02, 6.948000e+02,\n",
              "       8.963000e+02, 9.563000e+02, 9.971000e+02, 1.200500e+03,\n",
              "       1.725200e+03, 1.946000e+03, 1.319500e+03, 9.224000e+02,\n",
              "       1.185100e+03, 9.215000e+02, 4.409000e+02, 1.244300e+03,\n",
              "       6.813000e+02, 4.112000e+02, 4.473000e+02, 5.592000e+02,\n",
              "       4.599000e+02, 9.716000e+02, 6.941000e+02, 1.159600e+03,\n",
              "       8.847000e+02, 6.920000e+02, 8.980000e+02, 8.445000e+02,\n",
              "       7.965000e+02, 4.168000e+02, 5.902000e+02, 4.951000e+02,\n",
              "       5.158000e+02, 5.621000e+02, 8.093000e+02, 1.607300e+03,\n",
              "       1.001300e+03, 7.114000e+02, 4.442000e+02, 5.341000e+02,\n",
              "       7.982000e+02, 5.910000e+02, 4.432000e+02, 5.780000e+02,\n",
              "       7.651000e+02, 5.143000e+02, 1.150000e+03, 2.035300e+03,\n",
              "       7.022000e+02, 6.891000e+02, 5.532000e+02, 7.759000e+02,\n",
              "       6.217000e+02, 3.581000e+02, 9.393000e+02, 7.355000e+02,\n",
              "       9.246000e+02, 6.588000e+02, 1.055400e+03, 6.953000e+02,\n",
              "       4.600000e+02, 5.329000e+02, 4.626000e+02, 9.107000e+02,\n",
              "       8.896000e+02, 7.347000e+02, 1.304800e+03, 1.597300e+03,\n",
              "       1.917500e+03, 3.221500e+03, 9.198000e+02, 1.991600e+03,\n",
              "       5.948000e+02, 4.758000e+02, 7.151000e+02, 5.285000e+02,\n",
              "       5.800000e+02, 6.297000e+02, 5.982000e+02, 8.167000e+02,\n",
              "       6.408000e+02, 7.611000e+02, 4.621000e+02, 6.270000e+02,\n",
              "       7.225000e+02, 1.384200e+03, 4.415000e+02, 4.236000e+02,\n",
              "       4.967000e+02, 5.367000e+02, 4.294000e+02, 7.191000e+02,\n",
              "       3.945000e+02, 5.480000e+02, 2.633000e+02, 3.293000e+02,\n",
              "       3.787000e+02, 4.048000e+02, 6.593000e+02, 5.503000e+02,\n",
              "       4.856000e+02, 5.619000e+02, 5.123000e+02, 1.638000e+03,\n",
              "       8.844000e+02, 7.952000e+02, 1.049700e+03, 1.162100e+03,\n",
              "       8.584000e+02, 7.614000e+02, 7.499000e+02, 5.911000e+02,\n",
              "       6.928000e+02, 4.826000e+02, 8.440000e+02, 6.300000e+02,\n",
              "       5.671000e+02, 5.122000e+02, 4.622000e+02, 7.061000e+02,\n",
              "       5.339000e+02, 3.747000e+02, 3.738000e+02, 3.657000e+02,\n",
              "       3.139000e+02, 4.010000e+02, 2.755000e+02, 4.267000e+02,\n",
              "       5.746000e+02, 7.932000e+02, 4.469000e+02, 7.667000e+02,\n",
              "       1.865200e+03, 9.308000e+02, 5.027000e+02, 7.097000e+02,\n",
              "       5.639000e+02, 3.429000e+02, 4.788000e+02, 5.018000e+02,\n",
              "       4.103000e+02, 4.438000e+02, 1.414800e+03, 5.707000e+02,\n",
              "       3.913000e+02, 4.252000e+02, 5.263000e+02, 5.147000e+02,\n",
              "       4.565000e+02, 4.141000e+02, 4.645000e+02, 5.262000e+02,\n",
              "       4.873000e+02, 7.523000e+02, 6.325000e+02, 7.283000e+02,\n",
              "       6.411000e+02, 6.555000e+02, 4.889000e+02, 7.419000e+02,\n",
              "       7.134000e+02, 1.847200e+03, 3.107000e+02, 3.110000e+02,\n",
              "       9.277000e+02, 7.165000e+02, 7.970000e+02, 4.103000e+02,\n",
              "       5.036000e+02, 5.672000e+02, 1.504500e+03, 1.461900e+03,\n",
              "       1.142100e+03, 1.068500e+03, 7.002000e+02, 7.672000e+02,\n",
              "       8.551000e+02, 4.238000e+02, 5.757000e+02, 6.819000e+02,\n",
              "       5.403000e+02, 5.480000e+02, 5.645000e+02, 5.056000e+02,\n",
              "       4.356000e+02, 5.548000e+02, 4.966000e+02, 3.120000e+02,\n",
              "       4.147000e+02, 4.783000e+02, 4.170000e+02, 4.457000e+02,\n",
              "       4.592000e+02, 4.758000e+02, 6.932000e+02, 7.367000e+02,\n",
              "       3.531200e+03, 1.345600e+03, 9.676000e+02, 1.291600e+03,\n",
              "       1.524500e+03, 7.543000e+02, 1.089100e+03, 6.397000e+02,\n",
              "       8.184000e+02, 5.245000e+02, 8.190000e+02, 4.759000e+02,\n",
              "       1.011700e+03, 3.802000e+02, 2.840000e+02, 4.399000e+02,\n",
              "       3.529000e+02, 6.767000e+02, 8.987000e+02, 8.558000e+02,\n",
              "       6.047000e+02, 1.301200e+03, 9.283000e+02, 9.885000e+02,\n",
              "       9.434000e+02, 8.225000e+02, 1.168200e+03, 8.464000e+02,\n",
              "       8.266000e+02, 1.232700e+03, 9.376000e+02, 6.515000e+02,\n",
              "       6.146000e+02, 5.572000e+02, 3.619000e+02, 3.829000e+02,\n",
              "       4.299000e+02, 2.166000e+02, 4.664000e+02, 3.815500e+03,\n",
              "       9.837300e+03, 1.701700e+03, 1.234324e+05, 2.455900e+03,\n",
              "       1.082900e+04, 3.650100e+03, 4.710200e+03, 2.102700e+03,\n",
              "       3.472500e+03, 3.930100e+03, 7.943000e+02, 2.228600e+03,\n",
              "       1.299300e+03, 4.076600e+03, 4.597600e+03, 5.671300e+03,\n",
              "       2.362000e+03, 8.564000e+02, 7.750900e+03, 1.228200e+03,\n",
              "       3.793000e+02, 3.015600e+03, 7.129000e+02, 1.802400e+03,\n",
              "       2.631000e+03, 1.128200e+03, 1.861510e+04, 8.767000e+02,\n",
              "       3.061000e+02, 5.277400e+03, 2.238700e+03, 2.750500e+03,\n",
              "       7.599900e+03, 1.650400e+03, 4.641800e+03, 3.407100e+03,\n",
              "       2.364900e+03, 3.324500e+03, 1.131400e+03, 2.457200e+03,\n",
              "       1.151210e+04, 9.306600e+03, 1.483400e+03, 5.387000e+02,\n",
              "       1.301200e+03, 2.176100e+03, 6.894300e+03, 8.292000e+02,\n",
              "       4.083000e+02, 6.186700e+03, 1.808452e+05, 1.146750e+04,\n",
              "       2.971500e+03, 4.550800e+03, 6.433800e+03, 1.371800e+03,\n",
              "       1.793200e+03, 3.746700e+03, 5.888000e+03, 2.469700e+03,\n",
              "       6.127700e+03, 2.387000e+02, 1.234200e+03, 1.437610e+04,\n",
              "       1.433230e+04, 1.286000e+03, 1.511500e+03, 4.067930e+04,\n",
              "       3.385100e+03, 4.009700e+03, 3.824400e+03, 7.325600e+03,\n",
              "       2.670300e+03, 4.553000e+03, 6.710900e+03, 1.964100e+03,\n",
              "       5.372700e+03, 4.832000e+03, 3.055200e+03, 6.172000e+03,\n",
              "       2.232400e+03, 3.114680e+04, 8.229000e+02, 3.227300e+03,\n",
              "       1.261400e+03, 4.874200e+03, 1.014300e+03, 1.438400e+03,\n",
              "       1.357340e+04, 2.660500e+03, 3.139000e+02, 1.148600e+03,\n",
              "       5.875500e+03, 6.600000e+02, 2.239700e+03, 4.447400e+03,\n",
              "       4.514800e+03, 3.350600e+03, 8.281000e+02, 4.812000e+02,\n",
              "       1.388080e+04, 3.280200e+03, 1.574200e+03, 7.906000e+03,\n",
              "       5.985370e+04, 8.391000e+03, 1.051100e+03, 2.068100e+03,\n",
              "       5.765200e+03, 1.680700e+03, 1.467680e+04, 1.017380e+04,\n",
              "       3.025000e+02, 2.175500e+03, 1.448250e+04, 6.017600e+03,\n",
              "       3.974600e+03, 7.552600e+03, 4.372000e+02, 1.046000e+03,\n",
              "       5.799100e+03, 5.202600e+03, 1.370040e+04, 2.844100e+03,\n",
              "       5.354000e+02, 1.306900e+03, 3.059400e+03, 4.995000e+02,\n",
              "       1.502200e+03, 1.693300e+04, 1.199700e+03, 2.228100e+03,\n",
              "       1.421600e+03, 4.152100e+03, 1.910400e+03, 4.225500e+03,\n",
              "       3.850500e+03, 2.251160e+04, 1.009500e+03, 2.142300e+03,\n",
              "       7.507000e+02, 1.931500e+03, 3.508400e+03, 1.446400e+03,\n",
              "       3.781000e+03, 2.401200e+03, 4.110000e+03, 5.763000e+02,\n",
              "       3.299500e+03, 7.742000e+02, 1.206800e+03, 3.687800e+03,\n",
              "       4.480000e+02, 5.231900e+03, 2.347600e+03, 1.049400e+03,\n",
              "       1.030800e+03, 4.367600e+03, 1.062400e+03, 2.186500e+03,\n",
              "       7.744000e+02, 6.085580e+04, 3.432000e+03, 1.808240e+04,\n",
              "       3.215100e+03, 1.840900e+03, 6.749000e+02, 4.623700e+03,\n",
              "       2.536000e+03, 1.542910e+04, 1.873800e+03, 2.269800e+03,\n",
              "       3.553300e+03, 2.161800e+03, 1.789400e+03, 2.849400e+03,\n",
              "       1.651270e+04, 6.624000e+02, 1.636000e+03, 1.908400e+03,\n",
              "       4.030400e+03, 1.198500e+03, 6.707990e+04, 2.997000e+03,\n",
              "       1.680900e+04, 5.156400e+03, 1.653910e+04, 3.927000e+03,\n",
              "       3.908400e+03, 8.369000e+02, 2.533400e+03, 9.185600e+03,\n",
              "       2.098800e+03, 9.135000e+03, 3.905600e+03, 5.352000e+02,\n",
              "       1.157160e+04, 6.076000e+02, 1.652600e+03, 1.046280e+04,\n",
              "       3.658400e+03, 1.426600e+03, 4.278700e+03, 1.312090e+04,\n",
              "       3.729300e+03, 6.944100e+03, 2.904000e+03, 4.011600e+03,\n",
              "       7.389200e+03, 9.200000e+02, 2.795740e+04, 3.018000e+03,\n",
              "       2.705200e+03, 6.546000e+02, 7.948000e+02, 6.997800e+03,\n",
              "       1.048000e+03, 2.008500e+03, 6.155300e+03, 3.637000e+03,\n",
              "       3.252000e+02, 2.860000e+02, 1.793700e+03, 4.780090e+04,\n",
              "       2.353000e+03, 4.034900e+03, 4.444300e+03, 2.965200e+03,\n",
              "       9.955000e+02, 3.371000e+02, 2.728100e+03, 4.254200e+03,\n",
              "       1.876000e+03, 8.727000e+03, 1.728500e+03, 9.506200e+03,\n",
              "       3.332500e+03, 3.546050e+04, 2.816600e+03, 2.749000e+03,\n",
              "       1.490160e+04, 6.263800e+03, 2.827000e+03, 1.634900e+03,\n",
              "       2.122800e+03, 1.387040e+04, 1.271880e+04, 3.239900e+03,\n",
              "       6.135000e+02, 2.210900e+03, 3.959400e+03, 1.241400e+03,\n",
              "       4.493800e+03, 8.834000e+02, 1.656200e+03, 8.989000e+02,\n",
              "       2.887600e+03, 6.067100e+03, 2.408300e+03, 3.811400e+03,\n",
              "       1.332800e+03, 9.616700e+03, 1.856900e+03, 9.378000e+02,\n",
              "       9.625000e+03, 1.141850e+04, 8.823100e+03, 9.321000e+03,\n",
              "       3.858000e+02, 2.642400e+03, 2.860300e+03, 3.380500e+03,\n",
              "       2.975900e+03, 5.839300e+03, 1.518700e+03, 2.035200e+03,\n",
              "       1.859700e+03, 7.953300e+03, 6.109400e+03, 4.767000e+03,\n",
              "       4.970000e+02, 6.010500e+03, 4.900000e+02, 4.562000e+02,\n",
              "       3.680300e+03, 3.043700e+03, 3.880200e+03, 1.104860e+04,\n",
              "       3.424300e+03, 4.072100e+03, 1.389650e+04, 9.214200e+03,\n",
              "       3.840910e+04, 7.511700e+03, 7.210000e+02, 9.510000e+01,\n",
              "       3.441270e+04, 1.642300e+03, 3.611900e+03, 2.227600e+03,\n",
              "       2.171500e+03, 2.602200e+03, 5.625400e+03, 1.723960e+04,\n",
              "       1.164900e+03, 6.905600e+03, 1.197240e+04, 1.616100e+03,\n",
              "       5.385900e+03, 1.683700e+03, 1.884500e+03, 5.320100e+03,\n",
              "       1.967200e+03, 4.923600e+03, 2.000510e+04, 2.925800e+03,\n",
              "       8.769000e+02, 1.759600e+03, 1.670800e+03, 2.679500e+04,\n",
              "       1.278200e+03, 3.624100e+03, 3.731300e+03, 2.932200e+03,\n",
              "       2.040000e+03, 2.631700e+03, 2.041800e+03, 8.632000e+02,\n",
              "       1.519900e+03, 5.130400e+03, 6.585900e+03, 5.208600e+04,\n",
              "       3.470900e+03, 9.190800e+03, 4.619400e+03, 9.610000e+02,\n",
              "       6.121300e+03, 1.880200e+03, 1.169700e+03, 7.844500e+03,\n",
              "       4.890300e+03, 7.125000e+02, 8.735000e+02, 2.413400e+03,\n",
              "       1.126800e+03, 5.465000e+02, 3.001500e+03, 1.579100e+03,\n",
              "       5.405000e+02, 2.460200e+03, 1.233000e+03, 1.524100e+03,\n",
              "       1.457020e+04, 6.407000e+02, 3.252000e+03, 2.792600e+03,\n",
              "       5.103000e+02, 1.175480e+04, 1.018900e+03, 2.060800e+03,\n",
              "       3.964700e+03, 8.997000e+02, 9.040000e+02, 9.279000e+02,\n",
              "       1.574360e+04, 2.244000e+03, 1.425500e+03, 1.897000e+03,\n",
              "       1.637000e+04, 8.668000e+02, 1.402360e+04, 3.100900e+03,\n",
              "       9.204000e+02, 1.338000e+03, 2.947000e+02, 7.737000e+02,\n",
              "       5.771300e+03, 2.415600e+03, 2.680000e+02, 1.323500e+03,\n",
              "       5.119300e+03, 3.450400e+03, 1.572200e+03, 4.008800e+03,\n",
              "       4.659000e+02, 2.081200e+03, 6.668600e+03, 3.440300e+03,\n",
              "       1.111690e+04, 1.418700e+03, 2.033000e+03, 4.911500e+03,\n",
              "       3.473200e+03, 3.982800e+03, 8.322300e+03, 2.459700e+03,\n",
              "       2.913660e+04, 1.302400e+03, 2.928700e+03, 1.793700e+03,\n",
              "       8.490500e+03, 4.589100e+03, 1.935200e+03, 1.257500e+04,\n",
              "       1.734900e+03, 7.182800e+03, 2.330200e+03, 3.716000e+03,\n",
              "       1.316400e+03, 3.355000e+03, 8.833800e+03, 1.795200e+03,\n",
              "       7.026100e+03, 1.036930e+04, 3.630600e+03, 4.284000e+03,\n",
              "       8.785900e+03, 1.835200e+03, 1.553600e+03, 5.947000e+02,\n",
              "       1.219950e+04, 3.110300e+03, 5.894200e+03, 3.897200e+03,\n",
              "       3.433800e+03, 1.692500e+03, 1.289170e+04, 6.104100e+03,\n",
              "       2.018000e+04, 7.806000e+02, 2.042000e+03, 3.132800e+03,\n",
              "       1.545450e+04, 1.128000e+03, 1.863800e+03, 3.900000e+02,\n",
              "       2.652000e+03, 1.744900e+03, 1.196900e+03, 1.617660e+04,\n",
              "       9.544000e+02, 8.171000e+03, 3.118200e+03, 1.541800e+03,\n",
              "       3.933570e+04, 5.275000e+03, 1.010100e+03, 2.058800e+03,\n",
              "       7.020200e+03, 2.075310e+04, 4.277900e+03, 1.832400e+03,\n",
              "       2.071000e+03, 4.741400e+03, 6.275000e+02, 2.670400e+03,\n",
              "       2.555900e+03, 2.824700e+03, 2.780910e+04, 1.051400e+03,\n",
              "       1.347270e+04, 1.658740e+04, 2.962300e+03, 7.824000e+02,\n",
              "       3.974600e+03, 1.938700e+03, 4.186000e+03, 2.521200e+04,\n",
              "       1.932400e+03, 1.504762e+05, 2.476800e+03, 1.056210e+04,\n",
              "       2.613000e+03, 7.108800e+03, 2.040100e+03, 3.458700e+03,\n",
              "       3.252400e+03, 3.693000e+02, 3.008800e+03, 1.422200e+03,\n",
              "       5.112400e+03, 5.093200e+03, 4.573600e+03, 3.965300e+03,\n",
              "       5.834000e+02, 8.920500e+03, 1.377600e+03, 5.457000e+02,\n",
              "       5.421900e+03, 3.989000e+02, 3.052800e+03, 5.342100e+03,\n",
              "       9.276000e+02, 1.517320e+04, 7.141000e+02, 3.246000e+02,\n",
              "       7.882800e+03, 2.782200e+03, 2.575800e+03, 8.851900e+03,\n",
              "       2.532200e+03, 8.385500e+03, 4.986100e+03, 2.464500e+03,\n",
              "       4.087500e+03, 7.901000e+02, 2.705400e+03, 2.220100e+04,\n",
              "       1.136610e+04, 1.341000e+03, 3.011000e+02, 2.708800e+03,\n",
              "       1.843600e+03, 1.064120e+04, 7.000000e+02, 1.224700e+03,\n",
              "       8.867800e+03, 2.095213e+05, 5.993700e+03, 4.406600e+03,\n",
              "       7.023400e+03, 6.979200e+03, 1.093600e+03, 1.122200e+03,\n",
              "       3.276600e+03, 1.066340e+04, 4.899400e+03, 7.189300e+03,\n",
              "       1.988000e+02, 1.526800e+03, 1.697360e+04, 8.594200e+03,\n",
              "       2.654000e+03, 2.173700e+03, 6.686170e+04, 3.437400e+03,\n",
              "       5.517100e+03, 3.023700e+03, 5.697200e+03, 9.280400e+03,\n",
              "       4.510400e+03, 5.441000e+03, 2.116000e+03, 3.199900e+03,\n",
              "       6.705800e+03, 7.324400e+03, 6.662500e+03, 1.936000e+03,\n",
              "       2.869260e+04, 2.352700e+03, 1.000000e+01, 4.564300e+03,\n",
              "       1.412800e+03, 5.296800e+03, 3.386900e+03, 1.336700e+03,\n",
              "       1.774650e+04, 2.835000e+03, 5.110000e+02, 1.592300e+03,\n",
              "       4.288600e+03, 8.488000e+02, 4.227100e+03, 8.029400e+03,\n",
              "       6.313200e+03, 3.223400e+03, 5.152000e+02, 3.109000e+02,\n",
              "       1.008450e+04, 2.775800e+03, 2.497100e+03, 7.942400e+03,\n",
              "       4.512450e+04, 1.197120e+04, 1.455600e+03, 3.450100e+03,\n",
              "       8.993400e+03, 2.149700e+03, 7.512000e+03, 1.059370e+04,\n",
              "       1.200700e+03, 2.802200e+03, 2.506600e+04, 9.206100e+03,\n",
              "       3.007400e+03, 7.766400e+03, 2.836000e+02, 1.535200e+03,\n",
              "       1.025450e+04, 4.502200e+03, 1.030770e+04, 3.193000e+03,\n",
              "       8.574000e+02, 8.415000e+02, 3.456600e+03, 4.135000e+02,\n",
              "       1.027600e+03, 1.878380e+04, 9.571000e+02, 2.735200e+03,\n",
              "       2.127200e+03, 3.872300e+03, 1.541000e+03, 3.711400e+03,\n",
              "       6.632500e+03, 2.668310e+04, 1.098400e+03, 2.856000e+03,\n",
              "       9.488000e+02, 2.485100e+03, 3.246000e+03, 2.269800e+03,\n",
              "       2.707500e+03, 2.344800e+03, 4.450000e+03, 6.819000e+02,\n",
              "       4.545400e+03, 7.623000e+02, 2.014200e+03, 4.364200e+03,\n",
              "       5.423000e+02, 5.734800e+03, 2.946600e+03, 1.157100e+03,\n",
              "       9.292000e+02, 5.222400e+03, 1.393500e+03, 2.431800e+03,\n",
              "       6.426000e+02, 2.156202e+05, 2.898800e+03, 1.731300e+04,\n",
              "       2.493300e+03, 2.922100e+03, 2.143400e+03, 4.868800e+03,\n",
              "       2.674000e+03, 1.270070e+04, 2.914300e+03, 1.414100e+03,\n",
              "       2.981200e+03, 2.363100e+03, 1.064000e+03, 2.672000e+03,\n",
              "       2.219590e+04, 7.788000e+02, 2.520900e+03, 2.206600e+03,\n",
              "       3.272900e+03, 1.535400e+03, 6.455060e+04, 2.315000e+03,\n",
              "       2.078820e+04, 7.439200e+03, 1.806090e+04, 6.031900e+03,\n",
              "       6.003300e+03, 9.428000e+02, 1.288600e+03, 1.829710e+04,\n",
              "       1.719300e+03, 1.165940e+04, 6.883000e+03, 5.945000e+02,\n",
              "       1.898970e+04, 8.226000e+02, 1.646900e+03, 2.122660e+04,\n",
              "       2.300400e+03, 1.173300e+03, 4.180600e+03, 1.559430e+04,\n",
              "       3.195800e+03, 1.242400e+04, 2.805400e+03, 2.211800e+03,\n",
              "       6.479200e+03, 3.039400e+03, 2.883050e+04, 8.338900e+03,\n",
              "       1.854400e+03, 8.921000e+02, 5.095000e+02, 6.070600e+03,\n",
              "       3.828900e+03, 1.860700e+03, 6.841400e+03, 7.804000e+03,\n",
              "       2.236000e+02, 3.489000e+02, 3.039700e+03, 5.235770e+04,\n",
              "       3.142100e+03, 5.249500e+03, 6.134700e+03, 2.522700e+03,\n",
              "       1.120200e+03, 4.101000e+02, 2.247700e+03, 2.711400e+03,\n",
              "       2.186900e+03, 4.925700e+03, 1.510300e+03, 1.067310e+04,\n",
              "       9.555900e+03, 4.120830e+04, 5.012300e+03, 1.472000e+03,\n",
              "       1.666080e+04, 5.996200e+03, 2.136200e+03, 2.465900e+03,\n",
              "       1.689300e+03, 2.317240e+04, 1.990400e+04, 2.539600e+03,\n",
              "       1.129300e+03, 1.975600e+03, 9.097500e+03, 2.242100e+03,\n",
              "       6.205100e+03, 8.646000e+02, 7.092000e+02, 1.129600e+03,\n",
              "       3.449300e+03, 7.517100e+03, 2.356500e+03, 4.839200e+03,\n",
              "       3.684600e+03, 1.442840e+04, 1.463300e+03, 7.496000e+02,\n",
              "       1.335510e+04, 1.153830e+04, 9.931300e+03, 1.290600e+04,\n",
              "       3.699000e+02, 2.701000e+03, 2.627800e+03, 3.258700e+03,\n",
              "       5.206100e+03, 7.099000e+03, 7.795500e+03, 2.133200e+03,\n",
              "       1.982400e+03, 3.715280e+04, 6.568600e+03, 5.604300e+03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TWpN0nVTpUx"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ1FKEs-4btX",
        "colab_type": "text"
      },
      "source": [
        "### Normalize data\n",
        "- Normalize features\n",
        "- Use tf.math.l2_normalize to normalize features\n",
        "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Tfe00X78wB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e0fe5d9c-651a-47b9-a52d-b7387c08528f"
      },
      "source": [
        "feature = tf.math.l2_normalize(feature)\n",
        "feature"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=3774, shape=(1000, 4), dtype=float32, numpy=\n",
              "array([[0.02202894, 0.02245906, 0.02182905, 0.02253223],\n",
              "       [0.02235197, 0.02141321, 0.02140607, 0.02240551],\n",
              "       [0.0207707 , 0.02051548, 0.02051192, 0.02137037],\n",
              "       ...,\n",
              "       [0.00505436, 0.00513467, 0.00499903, 0.00514181],\n",
              "       [0.00785282, 0.0079956 , 0.0078082 , 0.00799738],\n",
              "       [0.00643931, 0.00662849, 0.00642682, 0.00664455]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmXUGc2oTspa"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJelDMpzxs0L",
        "colab_type": "text"
      },
      "source": [
        "### Define weight and bias\n",
        "- Initialize weight and bias with tf.zeros\n",
        "- tf.zeros is an initializer that generates tensors initialized to 0\n",
        "- Specify the value for shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o9RPWVTxs0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We are initializing weights and Bias with Zero\n",
        "w = tf.zeros(shape=(4,1))\n",
        "b = tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFrzUvpCx4Tv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "055525be-0111-48e8-9d0c-64a5ec732248"
      },
      "source": [
        "weight\n"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=3726, shape=(0,), dtype=int32, numpy=array([], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsQF1fNy7Cz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f9042d4-cb08-448c-b460-54807d8b0b05"
      },
      "source": [
        "bias"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=3766, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8a0wr94aTyjg"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMXXYdOSxs0Q",
        "colab_type": "text"
      },
      "source": [
        "### Get prediction\n",
        "- Define a function to get prediction\n",
        "- Approach: prediction = (X * W) + b; here is X is features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Cty1y0xs0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(x, w, b):\n",
        "    \n",
        "    xw_matmul = tf.matmul(x, w)\n",
        "    y = tf.add(xw_matmul, b)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmS3Tauxs0V",
        "colab_type": "text"
      },
      "source": [
        "### Calculate loss\n",
        "- Calculate loss using predictions\n",
        "- Define a function to calculate loss\n",
        "- We are calculating mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FRXmDd5xs0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y_actual, y_predicted):\n",
        "    \n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbBpnOtfT0wd"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkOzAUUsTmF_",
        "colab_type": "text"
      },
      "source": [
        "### Define a function to train the model\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R4uieGYLYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    #Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        \n",
        "        t.watch([w,b])\n",
        "        \n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    #Update Weights and Bias\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW4SEP8kT2ls"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeN0deOvT81N",
        "colab_type": "text"
      },
      "source": [
        "### Train the model for 100 epochs \n",
        "- Observe the training loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjkn4gUgLevE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1d04f7d-f355-4ef5-906f-d3b9d87238ae"
      },
      "source": [
        "for i in range(100):\n",
        "    \n",
        "    w, b = train(feature, labels, w, b)\n",
        "    print('Current Loss on iteration', i, loss(labels, prediction(feature, w, b)).numpy())"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Loss on iteration 0 235093000.0\n",
            "Current Loss on iteration 1 234061090.0\n",
            "Current Loss on iteration 2 233069760.0\n",
            "Current Loss on iteration 3 232117820.0\n",
            "Current Loss on iteration 4 231203820.0\n",
            "Current Loss on iteration 5 230325870.0\n",
            "Current Loss on iteration 6 229482770.0\n",
            "Current Loss on iteration 7 228672880.0\n",
            "Current Loss on iteration 8 227895330.0\n",
            "Current Loss on iteration 9 227148210.0\n",
            "Current Loss on iteration 10 226431040.0\n",
            "Current Loss on iteration 11 225742290.0\n",
            "Current Loss on iteration 12 225080690.0\n",
            "Current Loss on iteration 13 224445260.0\n",
            "Current Loss on iteration 14 223835140.0\n",
            "Current Loss on iteration 15 223249310.0\n",
            "Current Loss on iteration 16 222686460.0\n",
            "Current Loss on iteration 17 222145890.0\n",
            "Current Loss on iteration 18 221626900.0\n",
            "Current Loss on iteration 19 221128460.0\n",
            "Current Loss on iteration 20 220649740.0\n",
            "Current Loss on iteration 21 220189950.0\n",
            "Current Loss on iteration 22 219748180.0\n",
            "Current Loss on iteration 23 219324350.0\n",
            "Current Loss on iteration 24 218917100.0\n",
            "Current Loss on iteration 25 218525890.0\n",
            "Current Loss on iteration 26 218150160.0\n",
            "Current Loss on iteration 27 217789470.0\n",
            "Current Loss on iteration 28 217443180.0\n",
            "Current Loss on iteration 29 217110350.0\n",
            "Current Loss on iteration 30 216790780.0\n",
            "Current Loss on iteration 31 216483950.0\n",
            "Current Loss on iteration 32 216189300.0\n",
            "Current Loss on iteration 33 215906260.0\n",
            "Current Loss on iteration 34 215634340.0\n",
            "Current Loss on iteration 35 215373280.0\n",
            "Current Loss on iteration 36 215122780.0\n",
            "Current Loss on iteration 37 214881810.0\n",
            "Current Loss on iteration 38 214650690.0\n",
            "Current Loss on iteration 39 214428590.0\n",
            "Current Loss on iteration 40 214215200.0\n",
            "Current Loss on iteration 41 214010430.0\n",
            "Current Loss on iteration 42 213813600.0\n",
            "Current Loss on iteration 43 213624700.0\n",
            "Current Loss on iteration 44 213443440.0\n",
            "Current Loss on iteration 45 213269140.0\n",
            "Current Loss on iteration 46 213101860.0\n",
            "Current Loss on iteration 47 212941020.0\n",
            "Current Loss on iteration 48 212786700.0\n",
            "Current Loss on iteration 49 212638510.0\n",
            "Current Loss on iteration 50 212496160.0\n",
            "Current Loss on iteration 51 212359460.0\n",
            "Current Loss on iteration 52 212228140.0\n",
            "Current Loss on iteration 53 212102080.0\n",
            "Current Loss on iteration 54 211981040.0\n",
            "Current Loss on iteration 55 211864560.0\n",
            "Current Loss on iteration 56 211752960.0\n",
            "Current Loss on iteration 57 211645620.0\n",
            "Current Loss on iteration 58 211542620.0\n",
            "Current Loss on iteration 59 211443780.0\n",
            "Current Loss on iteration 60 211348620.0\n",
            "Current Loss on iteration 61 211257420.0\n",
            "Current Loss on iteration 62 211169890.0\n",
            "Current Loss on iteration 63 211085860.0\n",
            "Current Loss on iteration 64 211004800.0\n",
            "Current Loss on iteration 65 210927250.0\n",
            "Current Loss on iteration 66 210852660.0\n",
            "Current Loss on iteration 67 210781200.0\n",
            "Current Loss on iteration 68 210712320.0\n",
            "Current Loss on iteration 69 210646240.0\n",
            "Current Loss on iteration 70 210582880.0\n",
            "Current Loss on iteration 71 210521890.0\n",
            "Current Loss on iteration 72 210463500.0\n",
            "Current Loss on iteration 73 210407330.0\n",
            "Current Loss on iteration 74 210353420.0\n",
            "Current Loss on iteration 75 210301540.0\n",
            "Current Loss on iteration 76 210251800.0\n",
            "Current Loss on iteration 77 210203980.0\n",
            "Current Loss on iteration 78 210158180.0\n",
            "Current Loss on iteration 79 210114020.0\n",
            "Current Loss on iteration 80 210071820.0\n",
            "Current Loss on iteration 81 210031020.0\n",
            "Current Loss on iteration 82 209992060.0\n",
            "Current Loss on iteration 83 209954580.0\n",
            "Current Loss on iteration 84 209918530.0\n",
            "Current Loss on iteration 85 209883970.0\n",
            "Current Loss on iteration 86 209850830.0\n",
            "Current Loss on iteration 87 209818830.0\n",
            "Current Loss on iteration 88 209788140.0\n",
            "Current Loss on iteration 89 209758720.0\n",
            "Current Loss on iteration 90 209730560.0\n",
            "Current Loss on iteration 91 209703460.0\n",
            "Current Loss on iteration 92 209677360.0\n",
            "Current Loss on iteration 93 209652220.0\n",
            "Current Loss on iteration 94 209628220.0\n",
            "Current Loss on iteration 95 209605180.0\n",
            "Current Loss on iteration 96 209583020.0\n",
            "Current Loss on iteration 97 209561740.0\n",
            "Current Loss on iteration 98 209541230.0\n",
            "Current Loss on iteration 99 209521730.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanvD93FV0_k",
        "colab_type": "text"
      },
      "source": [
        "### Observe values of Weight\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSqpy4gtWaOD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5759a6c7-1059-4d22-c113-e34e4074a06d"
      },
      "source": [
        "#Check Weights and Bias\n",
        "print('Weights:\\n', w.numpy())\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights:\n",
            " [[54.94985 ]\n",
            " [55.04922 ]\n",
            " [54.392685]\n",
            " [55.49984 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y9KpRupYUEwy"
      },
      "source": [
        "### Observe values of Bias\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bhEWkGqHWohg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fdf0b911-f71a-4c13-eab4-a74c7cb255ef"
      },
      "source": [
        "print('Bias:\\n',b.numpy())"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bias:\n",
            " [4621.736]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sii9ZmyW-J_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}